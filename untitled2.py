# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_ivgEdq3vRujaJowJoZaTyn1ZDoYBYZ
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install rdkit-pypi deepchem

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import Descriptors
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import deepchem as dc
import os
os.environ["WANDB_DISABLED"] = "true"

# Upload using Colab UI or use Google Drive mount
df = pd.read_csv("/content/drive/MyDrive/Orphenix/orphan_disease_drug_dataset_100.csv")
disease_data = pd.read_csv("/content/drive/MyDrive/Orphenix/orphan_diseases.csv")
drug_data = pd.read_csv("/content/drive/MyDrive/Orphenix/drug_testing_data.csv")

# Fill NaNs with empty string and ensure all are strings
df["SMILES"] = df["SMILES"].fillna("").astype(str)

def featurize_smiles(smiles):
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return [0]*5
        return [
            Descriptors.MolWt(mol),
            Descriptors.MolLogP(mol),
            Descriptors.NumHDonors(mol),
            Descriptors.NumHAcceptors(mol),
            Descriptors.TPSA(mol)
        ]
    except:
        return [0]*5

df["smiles_features"] = df["SMILES"].apply(featurize_smiles)

smiles_df = pd.DataFrame(df["smiles_features"].to_list(), columns=[
    "MolWt", "MolLogP", "HDonors", "HAcceptors", "TPSA"
])

df = pd.concat([df, smiles_df], axis=1)

# Example: convert categorical text to numeric
coverage_map = {
    "Low": 0.3,
    "Medium": 0.6,
    "High": 0.9
}

df["Pathway Coverage"] = df["Pathway Coverage"].replace(coverage_map)

X = df[[
    "Pathway Coverage",
    "Binding Affinity",
    "MolWt", "MolLogP",
    "HDonors", "HAcceptors", "TPSA"
]]

y_compatibility = df["Compatibility Score"]
y_toxicity = df["Toxicity Score"]

# Normalize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split
X_train, X_test, y_c_train, y_c_test = train_test_split(X_scaled, y_compatibility, test_size=0.2, random_state=42)
_, _, y_t_train, y_t_test = train_test_split(X_scaled, y_toxicity, test_size=0.2, random_state=42)

# Compatibility model
model_compat = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1)
])
model_compat.compile(optimizer='adam', loss='mean_squared_error')
model_compat.fit(X_train, y_c_train, epochs=100, batch_size=16, validation_data=(X_test, y_c_test))

# Toxicity model
model_toxic = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1)
])
model_toxic.compile(optimizer='adam', loss='mean_squared_error')
model_toxic.fit(X_train, y_t_train, epochs=100, batch_size=16, validation_data=(X_test, y_t_test))

# ðŸ‘‡ Paste this entire block in a Colab code cell

import shap
import numpy as np
import pandas as pd

class DrugExplainer:
    def __init__(self, model_compat, model_toxic, scaler, feature_data):
        self.model_compat = model_compat
        self.model_toxic = model_toxic
        self.scaler = scaler
        self.X = feature_data

        # Model wrapper
        self.predict_fn = lambda x: self.model_compat.predict(self.scaler.transform(x))

        # SHAP explainer
        self.explainer = shap.Explainer(self.predict_fn, self.X)

        # Ideal reference from top-scoring samples
        self.ideal = self.X[self.X.index[
            self.model_compat.predict(self.scaler.transform(self.X))[:, 0] > 98
        ]].mean()

    def analyze(self, raw_features):
        raw_array = np.array(raw_features).reshape(1, -1)
        scaled_input = self.scaler.transform(raw_array)

        # Predictions
        compatibility = float(self.model_compat.predict(scaled_input)[0][0])
        toxicity = float(self.model_toxic.predict(scaled_input)[0][0])

        # SHAP explanation
        shap_values = self.explainer(raw_array)
        shap_df = pd.DataFrame({
            "feature": self.X.columns,
            "impact": shap_values.values[0]
        }).sort_values("impact", key=abs, ascending=False)

        # Top reasons
        reasons = []
        for _, row in shap_df.iterrows():
            if row["impact"] > 0:
                reasons.append(f"{row['feature']} positively contributed.")
            else:
                reasons.append(f"{row['feature']} negatively affected the score.")

        # What to improve
        diff = self.ideal - raw_array.flatten()
        improvement = []
        for i, val in enumerate(diff):
            if abs(val) > 0.1:
                direction = "increase" if val > 0 else "decrease"
                improvement.append(f"{direction} {self.X.columns[i]} by {abs(round(val, 2))}")

        return {
            "Predicted Compatibility Score": round(compatibility, 2),
            "Predicted Toxicity Score": round(toxicity, 2),
            "Why this Score": reasons[:3],
            "Why not 100%": improvement if improvement else ["Near perfect match."],
            "Modification Suggestions": improvement if improvement else ["Structure is strong."]
        }

# Step 1: Calculate top-scoring "ideal" samples (after training)
scores = model_compat.predict(scaler.transform(X))[:, 0]

# Select top 10 most compatible samples
top_indices = np.argsort(scores)[-10:]
ideal_features = X.iloc[top_indices].mean()

# Step 2: Initialize DrugExplainer with ideal features
explainer = DrugExplainer(
    model_compat=model_compat,
    model_toxic=model_toxic,
    scaler=scaler,
    feature_data=X
)
explainer.ideal = ideal_features  # assign manually if you're setting it outside __init__

# Step 3: Analyze one input sample
sample_input = X.iloc[5].tolist()  # You can change 5 to any row index
result = explainer.analyze(sample_input)

# Step 4: Print results
for key, value in result.items():
    print(f"\nðŸ”¹ {key}:\n{value}")

from sklearn.metrics import mean_squared_error, r2_score

# Compatibility Model Evaluation
y_c_pred = model_compat.predict(X_test).flatten()
print("\nðŸ”Ž Compatibility Model:")
print("MSE:", mean_squared_error(y_c_test, y_c_pred))
print("RÂ² Score:", r2_score(y_c_test, y_c_pred))

# Toxicity Model Evaluation
y_t_pred = model_toxic.predict(X_test).flatten()
print("\nðŸ’€ Toxicity Model:")
print("MSE:", mean_squared_error(y_t_test, y_t_pred))
print("RÂ² Score:", r2_score(y_t_test, y_t_pred))

shap_values = explainer.explainer(X.iloc[[5]])
shap.plots.waterfall(shap_values[0])

code = """
from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok
import numpy as np

app = Flask(__name__)
run_with_ngrok(app)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    features = data.get("features", [])
    if not features:
        return jsonify({"error": "No features provided"}), 400

    # Example: return dummy score (replace with real logic)
    score = sum(features) / len(features)
    return jsonify({"compatibility_score": score})

app.run()
"""

# Write to file
with open("backend.py", "w") as f:
    f.write(code)

from google.colab import files
files.download("backend.py")